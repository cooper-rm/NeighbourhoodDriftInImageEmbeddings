{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fef969e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6268b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import faiss\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import STL10\n",
    "from torchvision import models\n",
    "import os \n",
    "import json\n",
    "\n",
    "from src.config import load_config\n",
    "from src.metrics import (\n",
    "    compute_overlap_stats,\n",
    "    compute_distance_divergence_stats,\n",
    "    compute_lid_stats,\n",
    "    compute_barycenter_stats,\n",
    ")\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945cae59",
   "metadata": {},
   "source": [
    "# Load Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cc1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\n",
    "    \"../config/base.yaml\",\n",
    "    \"../config/data.yaml\",\n",
    "    \"../config/embedding.yaml\",\n",
    "    \"../config/search.yaml\",\n",
    "    \"../config/ann.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126b298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXPERIMENTS = cfg[\"experiment\"][\"num_experiments\"]\n",
    "BASE_SEED = cfg[\"experiment\"][\"seed\"]\n",
    "SAMPLE_SIZE = cfg[\"data\"][\"sample\"][\"size\"]\n",
    "\n",
    "M = cfg[\"ann\"][\"hnsw\"][\"m\"]\n",
    "EF_CONSTRUCTION = cfg[\"ann\"][\"hnsw\"][\"ef_construction\"]\n",
    "EF_SEARCH = cfg[\"ann\"][\"hnsw\"][\"ef_search\"]\n",
    "\n",
    "K_MIN = cfg[\"search\"][\"knn\"][\"k_min\"]\n",
    "K_MAX = cfg[\"search\"][\"knn\"][\"k_max\"]\n",
    "K_STEP = cfg[\"search\"][\"knn\"][\"k_step\"]\n",
    "K_VALUES = list(range(K_MIN, K_MAX + 1, K_STEP))\n",
    "\n",
    "RUNS_DIR = ROOT / Path(cfg[\"paths\"][\"runs_dir\"])\n",
    "RUNS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_PATH = ROOT / cfg[\"data\"][\"data_path\"]\n",
    "\n",
    "\n",
    "p = cfg[\"embedding\"][\"preprocessing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d27292",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize(p[\"resize\"]),\n",
    "    T.CenterCrop(p[\"center_crop\"]),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=p[\"normalize\"][\"mean\"],\n",
    "        std=p[\"normalize\"][\"std\"]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dea461",
   "metadata": {},
   "source": [
    "# Download The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6adf2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = STL10(\n",
    "    root=DATA_PATH,\n",
    "    split=\"unlabeled\",\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "FULL_DATASET_LEN = len(dataset)\n",
    "\n",
    "\n",
    "print(\"Full STL-10 size:\", FULL_DATASET_LEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e0f915",
   "metadata": {},
   "source": [
    "# Load the embedding model and remove the classification head. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c64993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ResNet-50 (ImageNet-trained weights)\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# Remove the final classification layer to expose penultimate embeddings\n",
    "embedding_model = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "embedding_model.to(DEVICE)\n",
    "embedding_model.eval()\n",
    "\n",
    "# Embedding extraction function\n",
    "def extract_embeddings(batch):\n",
    "    with torch.no_grad():\n",
    "        feats = embedding_model(batch.to(DEVICE))\n",
    "        # Output shape: (batch_size, 2048, 1, 1) → squeeze to (batch_size, 2048)\n",
    "        feats = feats.squeeze(-1).squeeze(-1)\n",
    "    return feats.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "print(\"Embedding dimension:\", 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26932e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "MIN_RUNS = 5\n",
    "WINDOW = 3                 # require stability for last 3 updates\n",
    "TOL_ABS = 1e-3             # absolute tolerance for curve change (tune)\n",
    "curve_history = []         # store per-run curves\n",
    "delta_history = []         # store mean-curve change per run\n",
    "\n",
    "def curve_delta(mean_prev, mean_curr, mode=\"max\"):\n",
    "    diff = np.abs(mean_curr - mean_prev)\n",
    "    if mode == \"max\":\n",
    "        return float(diff.max())\n",
    "    elif mode == \"rms\":\n",
    "        return float(np.sqrt(np.mean(diff**2)))\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'max' or 'rms'\")\n",
    "\n",
    "\n",
    "\n",
    "MAX_NUM_EXPERIMENTS = 100\n",
    "\n",
    "for i in range(MAX_NUM_EXPERIMENTS):\n",
    "    print(f\"\\n================ RUN {i} ================\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Run setup\n",
    "    # -----------------------------\n",
    "    run_cfg = deepcopy(cfg)\n",
    "    run_cfg[\"experiment\"][\"run_id\"] = i\n",
    "    \n",
    "    \n",
    "    run_cfg[\"data\"][\"sample\"][\"seed\"] = BASE_SEED + i # BASE SEED + i to get new sample each iter\n",
    "\n",
    "    run_id = f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_run{i:02d}\"\n",
    "    run_dir = ROOT / RUNS_DIR / run_id\n",
    "    run_dir.mkdir(exist_ok=False)\n",
    "\n",
    "    with open(run_dir / \"config.yaml\", \"w\") as f:\n",
    "        yaml.safe_dump(run_cfg, f)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Dataset subsample (per run)\n",
    "    # -----------------------------\n",
    "    rng = np.random.default_rng(run_cfg[\"data\"][\"sample\"][\"seed\"])\n",
    "    \n",
    "    # generates array of ints of size SAMPLE_SIZE within LEN of FULL_DATASET_LEN, \n",
    "    # without using same image mroe than 1 time.\n",
    "    indices = rng.choice(FULL_DATASET_LEN, SAMPLE_SIZE, replace=False)\n",
    "    \n",
    "    # collects a sample as a \"SUBSET\" from full dataset according to the random indices\n",
    "    # SUbset is an extension of DataLoader I think\n",
    "    dataset_subset = Subset(dataset, indices)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset_subset,\n",
    "        # TODO: update this from configs\n",
    "        batch_size=run_cfg[\"embedding\"][\"batch_size\"], \n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # Embeddings (per run)\n",
    "    # -----------------------------\n",
    "    emb_file = run_dir / \"embeddings.npy\"\n",
    "    emb_chunk_dir = run_dir / \"emb_chunks\"\n",
    "\n",
    "    if emb_file.exists():\n",
    "        embeddings = np.load(emb_file)\n",
    "    else:\n",
    "        emb_chunk_dir.mkdir(exist_ok=True)\n",
    "        chunks = []\n",
    "\n",
    "        for j, (batch, _) in enumerate(tqdm(loader)):\n",
    "            emb = extract_embeddings(batch) # (batch, 2048)\n",
    "            chunk_path = emb_chunk_dir / f\"chunk_{j:05d}.npy\"\n",
    "            np.save(chunk_path, emb)\n",
    "            chunks.append(chunk_path)\n",
    "\n",
    "        embeddings = np.vstack([np.load(p) for p in chunks])\n",
    "        np.save(emb_file, embeddings)\n",
    "\n",
    "    #  collect dimension of the embeddings \n",
    "    d = embeddings.shape[1]\n",
    "    print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # FAISS indices (per run)\n",
    "    # -----------------------------\n",
    "\n",
    "    # LINEAR SEARCH INDEX: \n",
    "    # Create a brute-force exact index that ranks \n",
    "    # neighbors by Euclidean distance.\n",
    "    index_exact = faiss.IndexFlatL2(d)\n",
    "\n",
    "    # Add embeddings to the index\n",
    "    index_exact.add(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "    # HNSW ANN SEARCH INDEX:     \n",
    "    # Build an HNSW graph index that ranks \n",
    "    # neighbors using Euclidean distance\n",
    "    index_ann = faiss.IndexHNSWFlat(d, M, faiss.METRIC_L2)\n",
    "\n",
    "    # set graph construction hyperparameter (larger = higher quality, slower build)\n",
    "    index_ann.hnsw.efConstruction = EF_CONSTRUCTION\n",
    "\n",
    "    # set search-time accuracy hyperparameter (larger = higher recall, slower search)\n",
    "    index_ann.hnsw.efSearch = EF_SEARCH\n",
    "\n",
    "    # add all embedding vectors to the ANN index\n",
    "    index_ann.add(embeddings)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Exact neighbors (per run)\n",
    "    # -----------------------------\n",
    "    exact_I_path = run_dir / \"exact_I.npy\"\n",
    "    exact_D_path = run_dir / \"exact_D.npy\"\n",
    "\n",
    "    if exact_I_path.exists() and exact_D_path.exists():\n",
    "        I_exact_full = np.load(exact_I_path)\n",
    "        D_exact_full = np.load(exact_D_path)\n",
    "    else:\n",
    "        D_exact_full, I_exact_full = index_exact.search(\n",
    "            embeddings, len(embeddings)\n",
    "        )\n",
    "        np.save(exact_I_path, I_exact_full)\n",
    "        np.save(exact_D_path, D_exact_full)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # k-sweep + metrics (per run)\n",
    "    # -----------------------------\n",
    "    results = {\n",
    "        \"k\": [],\n",
    "        \"mean_overlap\": [],\n",
    "        \"std_overlap\": [],\n",
    "        \"mean_dist_divergence\": [],\n",
    "        \"std_dist_divergence\": [],\n",
    "        \"mean_barycenter_shift\": [],\n",
    "        \"std_barycenter_shift\": [],\n",
    "        \"mean_lid_diff\": [],\n",
    "        \"std_lid_diff\": [],\n",
    "        \"lid_exact\": [],\n",
    "        \"lid_ann\": [],\n",
    "    }\n",
    "\n",
    "\n",
    "    for k in K_VALUES:\n",
    "        \n",
    "        # We ask for k+1 and drop the first value to prevent \n",
    "        # including the distance = 0 of a self returned query point. \n",
    "        # ---- Exact: slice and drop self-match ----\n",
    "        D_exact = D_exact_full[:, 1:k+1]\n",
    "        I_exact = I_exact_full[:, 1:k+1]\n",
    "\n",
    "        # ---- ANN: search k+1 and drop self-match ----\n",
    "        D_ann, I_ann = index_ann.search(embeddings, k + 1)\n",
    "        D_ann = D_ann[:, 1:]\n",
    "        I_ann = I_ann[:, 1:]\n",
    "\n",
    "        mean_ov, std_ov = compute_overlap_stats(I_exact, I_ann, k)\n",
    "    \n",
    "        mean_dist_divergence, std_dist_divergence = compute_distance_divergence_stats(D_exact, D_ann)\n",
    "        \n",
    "        bary_shift, std_bary_shift = compute_barycenter_stats(embeddings, I_exact, I_ann, D_exact)\n",
    "                \n",
    "        lid_stats = compute_lid_stats(D_exact, D_ann)\n",
    "\n",
    "        \n",
    "\n",
    "        results[\"k\"].append(k)\n",
    "        \n",
    "        results[\"mean_overlap\"].append(mean_ov)\n",
    "        results[\"std_overlap\"].append(std_ov)\n",
    "        \n",
    "        results[\"mean_dist_divergence\"].append(mean_dist_divergence)\n",
    "        results[\"std_dist_divergence\"].append(std_dist_divergence)\n",
    "        \n",
    "        results[\"mean_barycenter_shift\"].append(bary_shift)\n",
    "        results[\"std_barycenter_shift\"].append(std_bary_shift)\n",
    "        \n",
    "        results[\"mean_lid_diff\"].append(lid_stats[\"mean_lid_diff\"])\n",
    "        results[\"std_lid_diff\"].append(lid_stats[\"std_lid_diff\"])\n",
    "        results[\"lid_exact\"].append(lid_stats[\"mean_lid_exact\"])\n",
    "        results[\"lid_ann\"].append(lid_stats[\"mean_lid_ann\"])\n",
    "\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save results + plots (per run)\n",
    "    # -----------------------------\n",
    "    with open(run_dir / \"metrics.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    plots_dir = run_dir / \"plots\"\n",
    "    plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Overlap vs k\n",
    "    plt.figure()\n",
    "    plt.plot(results[\"k\"], results[\"mean_overlap\"])\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Mean Overlap\")\n",
    "    plt.title(\"Overlap vs k\")\n",
    "    plt.savefig(plots_dir / \"overlap_vs_k.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # LID difference vs k\n",
    "    plt.figure()\n",
    "    plt.plot(results[\"k\"], results[\"mean_lid_diff\"])\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Mean LID Difference (ANN − Exact)\")\n",
    "    plt.title(\"LID Difference vs k\")\n",
    "    plt.savefig(plots_dir / \"lid_vs_k.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Barycenter shift vs k\n",
    "    plt.figure()\n",
    "    plt.plot(results[\"k\"], results[\"mean_barycenter_shift\"])\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Mean Normalized Barycenter Shift\")\n",
    "    plt.title(\"Barycenter Shift vs k\")\n",
    "    plt.savefig(plots_dir / \"barycenter_shift_vs_k.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # Store this run's curve\n",
    "    curve = np.array(results[\"mean_dist_divergence\"], dtype=float)\n",
    "\n",
    "    if not np.isfinite(curve).all():\n",
    "        print(\"Warning: non-finite divergence values detected, skipping convergence check.\")\n",
    "    else:\n",
    "        curve_history.append(curve)\n",
    "\n",
    "\n",
    "    # Once we have at least 2 runs, track mean-curve change\n",
    "    if len(curve_history) >= 2:\n",
    "        mean_prev = np.mean(curve_history[:-1], axis=0)\n",
    "        mean_curr = np.mean(curve_history, axis=0)\n",
    "\n",
    "        delta = curve_delta(mean_prev, mean_curr, mode=\"max\")  # or \"rms\"\n",
    "        delta_history.append(delta)\n",
    "\n",
    "        print(f\"Mean distance-divergence curve change Δ = {delta:.6g}\")\n",
    "\n",
    "    # Stop rule: stable for WINDOW consecutive updates (after MIN_RUNS)\n",
    "    if len(curve_history) >= MIN_RUNS and len(delta_history) >= WINDOW:\n",
    "        if all(d < TOL_ABS for d in delta_history[-WINDOW:]):\n",
    "            print(f\"\\nDistance-divergence curve stabilized after {len(curve_history)} runs.\")\n",
    "            break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "similarity-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
