{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fef969e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88b64463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=RuntimeWarning,\n",
    "    message=\".*overflow.*|.*invalid value.*\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6268b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import faiss\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import STL10\n",
    "from torchvision import models\n",
    "import os \n",
    "import json\n",
    "\n",
    "from src.config import load_config\n",
    "from src.metrics import (\n",
    "    compute_overlap_stats,\n",
    "    compute_distance_stats, \n",
    "    compute_lid_stats,\n",
    "    compute_barycenter_stats,\n",
    ")\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945cae59",
   "metadata": {},
   "source": [
    "# Load Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67cc1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config(\n",
    "    \"../config/base.yaml\",\n",
    "    \"../config/data.yaml\",\n",
    "    \"../config/embedding.yaml\",\n",
    "    \"../config/search.yaml\",\n",
    "    \"../config/ann.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "126b298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXPERIMENTS = cfg[\"experiment\"][\"num_experiments\"]\n",
    "BASE_SEED = cfg[\"experiment\"][\"seed\"]\n",
    "SAMPLE_SIZE = cfg[\"data\"][\"sample\"][\"size\"]\n",
    "\n",
    "M = cfg[\"ann\"][\"hnsw\"][\"m\"]\n",
    "EF_CONSTRUCTION = cfg[\"ann\"][\"hnsw\"][\"ef_construction\"]\n",
    "EF_SEARCH = cfg[\"ann\"][\"hnsw\"][\"ef_search\"]\n",
    "\n",
    "K_MIN = cfg[\"search\"][\"knn\"][\"k_min\"]\n",
    "K_MAX = cfg[\"search\"][\"knn\"][\"k_max\"]\n",
    "K_STEP = cfg[\"search\"][\"knn\"][\"k_step\"]\n",
    "K_VALUES = list(range(K_MIN, K_MAX + 1, K_STEP))\n",
    "\n",
    "RUNS_DIR = ROOT / Path(cfg[\"paths\"][\"runs_dir\"])\n",
    "RUNS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_PATH = ROOT / cfg[\"data\"][\"data_path\"]\n",
    "\n",
    "\n",
    "p = cfg[\"embedding\"][\"preprocessing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63d27292",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize(p[\"resize\"]),\n",
    "    T.CenterCrop(p[\"center_crop\"]),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=p[\"normalize\"][\"mean\"],\n",
    "        std=p[\"normalize\"][\"std\"]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dea461",
   "metadata": {},
   "source": [
    "# Download The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6adf2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full STL-10 size: 100000\n"
     ]
    }
   ],
   "source": [
    "dataset = STL10(\n",
    "    root=DATA_PATH,\n",
    "    split=\"unlabeled\",\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "FULL_DATASET_LEN = len(dataset)\n",
    "\n",
    "\n",
    "print(\"Full STL-10 size:\", FULL_DATASET_LEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e0f915",
   "metadata": {},
   "source": [
    "# Load the embedding model and remove the classification head. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31c64993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 2048\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained ResNet-50 (ImageNet-trained weights)\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# Remove the final classification layer to expose penultimate embeddings\n",
    "embedding_model = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "embedding_model.to(DEVICE)\n",
    "embedding_model.eval()\n",
    "\n",
    "# Embedding extraction function\n",
    "def extract_embeddings(batch):\n",
    "    with torch.no_grad():\n",
    "        feats = embedding_model(batch.to(DEVICE))\n",
    "        # Output shape: (batch_size, 2048, 1, 1) → squeeze to (batch_size, 2048)\n",
    "        feats = feats.squeeze(-1).squeeze(-1)\n",
    "    return feats.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "print(\"Embedding dimension:\", 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26932e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ RUN 0 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:12<00:00, 24.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10000, 2048)\n",
      "\n",
      "================ RUN 1 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:12<00:00, 25.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10000, 2048)\n",
      "Run 2 | Δ(mean) = 5.52118e-05 | Δ_rel = 0.00478703 | ||std|| = 0.000281526\n",
      "\n",
      "================ RUN 2 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:12<00:00, 24.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10000, 2048)\n",
      "Run 3 | Δ(mean) = 4.24263e-05 | Δ_rel = 0.00617811 | ||std|| = 0.00033848\n",
      "\n",
      "================ RUN 3 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:12<00:00, 25.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10000, 2048)\n",
      "Run 4 | Δ(mean) = 2.30588e-05 | Δ_rel = 0.00326518 | ||std|| = 0.000347878\n",
      "\n",
      "================ RUN 4 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:12<00:00, 25.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10000, 2048)\n",
      "Run 5 | Δ(mean) = 1.13401e-05 | Δ_rel = 0.00163026 | ||std|| = 0.000328834\n",
      "\n",
      "================ RUN 5 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:12<00:00, 25.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10000, 2048)\n",
      "Run 6 | Δ(mean) = 1.18127e-05 | Δ_rel = 0.00171005 | ||std|| = 0.000324745\n",
      "\n",
      "================ RUN 6 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:12<00:00, 25.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10000, 2048)\n",
      "Run 7 | Δ(mean) = 5.80987e-06 | Δ_rel = 0.00083477 | ||std|| = 0.000307976\n",
      "\n",
      "================ RUN 7 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:12<00:00, 25.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10000, 2048)\n",
      "Run 8 | Δ(mean) = 1.87733e-06 | Δ_rel = 0.000270609 | ||std|| = 0.000289026\n",
      "\n",
      "================ RUN 8 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:12<00:00, 25.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10000, 2048)\n",
      "Run 9 | Δ(mean) = 1.52289e-05 | Δ_rel = 0.00219373 | ||std|| = 0.000339223\n",
      "\n",
      "================ RUN 9 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:13<00:00, 23.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10000, 2048)\n",
      "Run 10 | Δ(mean) = 1.20694e-05 | Δ_rel = 0.00172308 | ||std|| = 0.000363878\n",
      "\n",
      "Distance-divergence curve stabilized after 10 runs.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "MIN_RUNS = 10\n",
    "WINDOW = 5             # require stability for last 3 updates\n",
    "TOL_ABS = 1e-3        # RMS mean-curve change tolerance\n",
    "STD_TOL = 1e-3\n",
    "curve_history = []         # store per-run curves\n",
    "delta_history = []         # store mean-curve change per run\n",
    "std_norm = np.inf\n",
    "\n",
    "MAX_NUM_EXPERIMENTS = 100\n",
    "\n",
    "def to_json_safe(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: to_json_safe(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [to_json_safe(v) for v in obj]\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.floating, np.integer)):\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "    \n",
    "for i in range(MAX_NUM_EXPERIMENTS):\n",
    "    print(f\"\\n================ RUN {i} ================\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Run setup\n",
    "    # -----------------------------\n",
    "    run_cfg = deepcopy(cfg)\n",
    "    run_cfg[\"experiment\"][\"run_id\"] = i\n",
    "    \n",
    "    \n",
    "    run_cfg[\"data\"][\"sample\"][\"seed\"] = BASE_SEED + i # BASE SEED + i to get new sample each iter\n",
    "\n",
    "    run_id = f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_run{i:02d}\"\n",
    "    run_dir = ROOT / RUNS_DIR / run_id\n",
    "    run_dir.mkdir(exist_ok=False)\n",
    "\n",
    "    with open(run_dir / \"config.yaml\", \"w\") as f:\n",
    "        yaml.safe_dump(run_cfg, f)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Dataset subsample (per run)\n",
    "    # -----------------------------\n",
    "    rng = np.random.default_rng(run_cfg[\"data\"][\"sample\"][\"seed\"])\n",
    "    \n",
    "    # generates array of ints of size SAMPLE_SIZE within LEN of FULL_DATASET_LEN, \n",
    "    # without using same image mroe than 1 time.\n",
    "    indices = rng.choice(FULL_DATASET_LEN, SAMPLE_SIZE, replace=False)\n",
    "    \n",
    "    # collects a sample as a \"SUBSET\" from full dataset according to the random indices\n",
    "    # SUbset is an extension of DataLoader I think\n",
    "    dataset_subset = Subset(dataset, indices)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset_subset,\n",
    "        # TODO: update this from configs\n",
    "        batch_size=run_cfg[\"embedding\"][\"batch_size\"], \n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # Embeddings (per run)\n",
    "    # -----------------------------\n",
    "    emb_file = run_dir / \"embeddings.npy\"\n",
    "    emb_chunk_dir = run_dir / \"emb_chunks\"\n",
    "\n",
    "    if emb_file.exists():\n",
    "        embeddings = np.load(emb_file)\n",
    "    else:\n",
    "        emb_chunk_dir.mkdir(exist_ok=True)\n",
    "        chunks = []\n",
    "\n",
    "        for j, (batch, _) in enumerate(tqdm(loader)):\n",
    "            emb = extract_embeddings(batch) # (batch, 2048)\n",
    "            chunk_path = emb_chunk_dir / f\"chunk_{j:05d}.npy\"\n",
    "            np.save(chunk_path, emb)\n",
    "            chunks.append(chunk_path)\n",
    "\n",
    "        embeddings = np.vstack([np.load(p) for p in chunks])\n",
    "        np.save(emb_file, embeddings)\n",
    "\n",
    "    #  collect dimension of the embeddings \n",
    "    d = embeddings.shape[1]\n",
    "    print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # FAISS indices (per run)\n",
    "    # -----------------------------\n",
    "\n",
    "    # LINEAR SEARCH INDEX: \n",
    "    # Create a brute-force exact index that ranks \n",
    "    # neighbors by Euclidean distance.\n",
    "    index_exact = faiss.IndexFlatL2(d)\n",
    "\n",
    "    # Add embeddings to the index\n",
    "    index_exact.add(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "    # HNSW ANN SEARCH INDEX:     \n",
    "    # Build an HNSW graph index that ranks \n",
    "    # neighbors using Euclidean distance\n",
    "    index_ann = faiss.IndexHNSWFlat(d, M, faiss.METRIC_L2)\n",
    "\n",
    "    # set graph construction hyperparameter (larger = higher quality, slower build)\n",
    "    index_ann.hnsw.efConstruction = EF_CONSTRUCTION\n",
    "\n",
    "    # set search-time accuracy hyperparameter (larger = higher recall, slower search)\n",
    "    index_ann.hnsw.efSearch = EF_SEARCH\n",
    "\n",
    "    # add all embedding vectors to the ANN index\n",
    "    index_ann.add(embeddings)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Exact neighbors (per run)\n",
    "    # -----------------------------\n",
    "    exact_I_path = run_dir / \"exact_I.npy\"\n",
    "    exact_D_path = run_dir / \"exact_D.npy\"\n",
    "\n",
    "    if exact_I_path.exists() and exact_D_path.exists():\n",
    "        I_exact_full = np.load(exact_I_path)\n",
    "        D_exact_full = np.load(exact_D_path)\n",
    "    else:\n",
    "        D_exact_full, I_exact_full = index_exact.search(\n",
    "            embeddings, len(embeddings)\n",
    "        )\n",
    "        np.save(exact_I_path, I_exact_full)\n",
    "        np.save(exact_D_path, D_exact_full)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # k-sweep + metrics (per run)\n",
    "    # -----------------------------\n",
    "    results = {\n",
    "        \"k\": [],\n",
    "        \"mean_overlap\": [],\n",
    "        \"std_overlap\": [],\n",
    "        \"mean_exact_dist\": [],\n",
    "        \"std_exact_dist\": [],\n",
    "        \"mean_ann_dist\": [],\n",
    "        \"std_ann_dist\": [],\n",
    "        \"mean_barycenter_shift\": [],\n",
    "        \"std_barycenter_shift\": [],\n",
    "        \"mean_lid_diff\": [],\n",
    "        \"std_lid_diff\": [],\n",
    "        \"lid_exact\": [],\n",
    "        \"lid_ann\": [],\n",
    "    }\n",
    "\n",
    "\n",
    "    for k in K_VALUES:\n",
    "        \n",
    "        # We ask for k+1 and drop the first value to prevent \n",
    "        # including the distance = 0 of a self returned query point. \n",
    "        # ---- Exact: slice and drop self-match ----\n",
    "        D_exact = D_exact_full[:, 1:k+1]\n",
    "        I_exact = I_exact_full[:, 1:k+1]\n",
    "\n",
    "        # ---- ANN: search k+1 and drop self-match ----\n",
    "        D_ann, I_ann = index_ann.search(embeddings, k + 1)\n",
    "        D_ann = D_ann[:, 1:]\n",
    "        I_ann = I_ann[:, 1:]\n",
    "\n",
    "        mean_ov, std_ov = compute_overlap_stats(I_exact, I_ann, k)\n",
    "    \n",
    "        dist_stats = compute_distance_stats(D_exact, D_ann)\n",
    "        \n",
    "        bary_shift, std_bary_shift = compute_barycenter_stats(embeddings, I_exact, I_ann, D_exact)\n",
    "                \n",
    "        lid_stats = compute_lid_stats(D_exact, D_ann)\n",
    "\n",
    "        \n",
    "\n",
    "        results[\"k\"].append(k)\n",
    "        \n",
    "        results[\"mean_overlap\"].append(mean_ov)\n",
    "        results[\"std_overlap\"].append(std_ov)\n",
    "        \n",
    "        results[\"mean_exact_dist\"].append(dist_stats[\"mean_exact_dist\"])\n",
    "        results[\"std_exact_dist\"].append(dist_stats[\"std_exact_dist\"])\n",
    "        results[\"mean_ann_dist\"].append(dist_stats[\"mean_ann_dist\"])\n",
    "        results[\"std_ann_dist\"].append(dist_stats[\"std_ann_dist\"])\n",
    "\n",
    "                \n",
    "        results[\"mean_barycenter_shift\"].append(bary_shift)\n",
    "        results[\"std_barycenter_shift\"].append(std_bary_shift)\n",
    "        \n",
    "        results[\"mean_lid_diff\"].append(lid_stats[\"mean_lid_diff\"])\n",
    "        results[\"std_lid_diff\"].append(lid_stats[\"std_lid_diff\"])\n",
    "        results[\"lid_exact\"].append(lid_stats[\"mean_lid_exact\"])\n",
    "        results[\"lid_ann\"].append(lid_stats[\"mean_lid_ann\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save results + plots (per run)\n",
    "    # -----------------------------\n",
    "    with open(run_dir / \"metrics.json\", \"w\") as f:\n",
    "        json.dump(to_json_safe(results), f, indent=2)\n",
    "\n",
    "\n",
    "    plots_dir = run_dir / \"plots\"\n",
    "    plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Overlap vs k\n",
    "    plt.figure()\n",
    "    plt.plot(results[\"k\"], results[\"mean_overlap\"])\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Mean Overlap\")\n",
    "    plt.title(\"Overlap vs k\")\n",
    "    plt.savefig(plots_dir / \"overlap_vs_k.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # LID difference vs k\n",
    "    plt.figure()\n",
    "    plt.plot(results[\"k\"], results[\"mean_lid_diff\"])\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Mean LID Difference (ANN − Exact)\")\n",
    "    plt.title(\"LID Difference vs k\")\n",
    "    plt.savefig(plots_dir / \"lid_vs_k.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Barycenter shift vs k\n",
    "    plt.figure()\n",
    "    plt.plot(results[\"k\"], results[\"mean_barycenter_shift\"])\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Mean Normalized Barycenter Shift\")\n",
    "    plt.title(\"Barycenter Shift vs k\")\n",
    "    plt.savefig(plots_dir / \"barycenter_shift_vs_k.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(results[\"k\"], results[\"mean_exact_dist\"], label=\"Exact\")\n",
    "    plt.plot(results[\"k\"], results[\"mean_ann_dist\"], label=\"ANN\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Mean k-NN Distance\")\n",
    "    plt.title(\"Mean Neighborhood Radius vs k\")\n",
    "    plt.legend()\n",
    "    plt.savefig(plots_dir / \"mean_distance_vs_k.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    curve = np.asarray(results[\"mean_ann_dist\"], dtype=float)\n",
    "    curve_history.append(curve)\n",
    "\n",
    "    if len(curve_history) >= 2:\n",
    "        min_len = min(len(c) for c in curve_history)\n",
    "        curves_aligned = np.stack([c[:min_len] for c in curve_history])\n",
    "\n",
    "        mean_prev = np.mean(curves_aligned[:-1], axis=0)\n",
    "        mean_curr = np.mean(curves_aligned, axis=0)\n",
    "\n",
    "        delta = float(np.sqrt(np.mean((mean_curr - mean_prev) ** 2)))\n",
    "        delta_history.append(delta)\n",
    "\n",
    "        curve_std = np.std(curves_aligned, axis=0)\n",
    "        std_norm = float(np.linalg.norm(curve_std))\n",
    "\n",
    "        mean_norm = float(np.linalg.norm(mean_prev)) + 1e-8\n",
    "        delta_rel = float(delta / mean_norm)\n",
    "\n",
    "        print(\n",
    "            f\"Run {len(curve_history)} | \"\n",
    "            f\"Δ(mean) = {delta:.6g} | \"\n",
    "            f\"Δ_rel = {delta_rel:.6g} | \"\n",
    "            f\"||std|| = {std_norm:.6g}\"\n",
    "        )\n",
    "\n",
    "    if (\n",
    "        len(curve_history) >= MIN_RUNS\n",
    "        and len(delta_history) >= WINDOW\n",
    "        and all(d < TOL_ABS for d in delta_history[-WINDOW:])\n",
    "        and (std_norm / (mean_norm + 1e-8)) < STD_TOL\n",
    "\n",
    "    ):\n",
    "        print(\n",
    "            f\"\\nANN mean-distance curve stabilized after \"\n",
    "            f\"{len(curve_history)} runs.\"\n",
    "        )\n",
    "        break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "similarity-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
