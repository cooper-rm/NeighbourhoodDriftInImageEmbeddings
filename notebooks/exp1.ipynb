{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62407a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=RuntimeWarning,\n",
    "    message=\".*overflow.*|.*invalid value.*\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eff816d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import faiss\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import STL10\n",
    "from torchvision import models\n",
    "import os \n",
    "import json\n",
    "\n",
    "from src.config import load_config\n",
    "from src.metrics import (\n",
    "    compute_overlap_stats,\n",
    "    compute_distance_stats, \n",
    "    compute_lid_stats,\n",
    "    compute_barycenter_stats,\n",
    ")\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7581792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment': {'name': 'neighborhood_analysis',\n",
       "  'base_seed': 42,\n",
       "  'max_runs': 25,\n",
       "  'min_runs': 10,\n",
       "  'convergence': {'enabled': True,\n",
       "   'window': 5,\n",
       "   'tolerance_abs': '1e-3',\n",
       "   'tolerance_std': '1e-4'}},\n",
       " 'data': {'dataset': 'STL-10',\n",
       "  'total_size': 100000,\n",
       "  'sample': {'enabled': True,\n",
       "   'subset_size': 10000,\n",
       "   'sampling': 'random',\n",
       "   'replacement': False}},\n",
       " 'embedding': {'model': 'resnet50',\n",
       "  'pretrained': 'imagenet1k',\n",
       "  'output_dim': 2048,\n",
       "  'batch_size': 32,\n",
       "  'device': 'cuda',\n",
       "  'normalize': False},\n",
       " 'search': {'distance_metric': 'l2',\n",
       "  'exact': {'enabled': True},\n",
       "  'ann': {'enabled': True,\n",
       "   'algorithm': 'hnsw',\n",
       "   'parameters': {'M': 32, 'efConstruction': 200, 'efSearch': 50}}},\n",
       " 'evaluation': {'min_k': 10,\n",
       "  'max_k': 500,\n",
       "  'k_step': 10,\n",
       "  'metrics': {'neighborhood_overlap': True,\n",
       "   'average_neighbor_distance': True,\n",
       "   'barycenter_shift': True,\n",
       "   'local_intrinsic_dimensionality': True}},\n",
       " 'ef_search_study': {'enabled': True, 'values': [20, 50, 100]},\n",
       " 'ef_construction_study': {'enabled': True,\n",
       "  'values': [50, 100, 200, 300, 400]},\n",
       " 'alpha_study': {'enabled': True,\n",
       "  'single_run': True,\n",
       "  'alpha_values': [0.25, 0.5, 1.0, 2.0, 4.0]},\n",
       " 'output': {'save_embeddings': True,\n",
       "  'save_neighbors': False,\n",
       "  'save_metrics': True,\n",
       "  'save_plots': True,\n",
       "  'format': 'json'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to config\n",
    "CONFIG_PATH = Path(\"../config/config.yaml\")\n",
    "\n",
    "# Load YAML config\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19702fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_seed = cfg[\"experiment\"][\"base_seed\"]\n",
    "max_runs = cfg[\"experiment\"][\"max_runs\"]\n",
    "min_runs = cfg[\"experiment\"][\"min_runs\"]\n",
    "\n",
    "dataset_name = cfg[\"data\"][\"dataset\"]\n",
    "subset_size = cfg[\"data\"][\"sample\"][\"subset_size\"]\n",
    "replacement = cfg[\"data\"][\"sample\"][\"replacement\"]\n",
    "\n",
    "embedding_cfg = cfg[\"embedding\"]\n",
    "\n",
    "model_name = embedding_cfg[\"model\"]\n",
    "batch_size = embedding_cfg[\"batch_size\"]\n",
    "device = embedding_cfg[\"device\"]\n",
    "\n",
    "ann_cfg = cfg[\"search\"][\"ann\"][\"parameters\"]\n",
    "\n",
    "M = ann_cfg[\"M\"]\n",
    "ef_construction = ann_cfg[\"efConstruction\"]\n",
    "ef_search = ann_cfg[\"efSearch\"]\n",
    "\n",
    "\n",
    "\n",
    "eval_cfg = cfg[\"evaluation\"]\n",
    "\n",
    "K_VALUES = list(\n",
    "    range(\n",
    "        eval_cfg[\"min_k\"],\n",
    "        eval_cfg[\"max_k\"] + 1,\n",
    "        eval_cfg[\"k_step\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "K_VALUES[:10], K_VALUES[-5:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize(p[\"resize\"]),\n",
    "    T.CenterCrop(p[\"center_crop\"]),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=p[\"normalize\"][\"mean\"],\n",
    "        std=p[\"normalize\"][\"std\"]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d8ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = STL10(\n",
    "    root=DATA_PATH,\n",
    "    split=\"unlabeled\",\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "FULL_DATASET_LEN = len(dataset)\n",
    "\n",
    "\n",
    "print(\"Full STL-10 size:\", FULL_DATASET_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54fc89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ResNet-50 (ImageNet-trained weights)\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# Remove the final classification layer to expose penultimate embeddings\n",
    "embedding_model = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "embedding_model.to(DEVICE)\n",
    "embedding_model.eval()\n",
    "\n",
    "# Embedding extraction function\n",
    "def extract_embeddings(batch):\n",
    "    with torch.no_grad():\n",
    "        feats = embedding_model(batch.to(DEVICE))\n",
    "        # Output shape: (batch_size, 2048, 1, 1) → squeeze to (batch_size, 2048)\n",
    "        feats = feats.squeeze(-1).squeeze(-1)\n",
    "    return feats.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "print(\"Embedding dimension:\", 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d96539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 49.3M/2.64G [00:06<05:28, 7.89MB/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 78\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# ======================================================\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# ---------------- ONE-TIME SETUP ----------------------\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# ======================================================\u001b[39;00m\n\u001b[1;32m     71\u001b[0m transform \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     72\u001b[0m     T\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m96\u001b[39m),\n\u001b[1;32m     73\u001b[0m     T\u001b[38;5;241m.\u001b[39mCenterCrop(\u001b[38;5;241m96\u001b[39m),\n\u001b[1;32m     74\u001b[0m     T\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     75\u001b[0m     T\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), std\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m)),\n\u001b[1;32m     76\u001b[0m ])\n\u001b[0;32m---> 78\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSTL10\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munlabeled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m FULL_DATASET_LEN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull STL-10 size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, FULL_DATASET_LEN)\n",
      "File \u001b[0;32m~/miniconda3/envs/similarity-exp/lib/python3.10/site-packages/torchvision/datasets/stl10.py:61\u001b[0m, in \u001b[0;36mSTL10.__init__\u001b[0;34m(self, root, split, folds, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfolds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_folds(folds)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/similarity-exp/lib/python3.10/site-packages/torchvision/datasets/stl10.py:158\u001b[0m, in \u001b[0;36mSTL10.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgz_md5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity()\n",
      "File \u001b[0;32m~/miniconda3/envs/similarity-exp/lib/python3.10/site-packages/torchvision/datasets/utils.py:388\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[1;32m    386\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[0;32m--> 388\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    391\u001b[0m extract_archive(archive, extract_root, remove_finished)\n",
      "File \u001b[0;32m~/miniconda3/envs/similarity-exp/lib/python3.10/site-packages/torchvision/datasets/utils.py:127\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# download the file\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/similarity-exp/lib/python3.10/site-packages/torchvision/datasets/utils.py:30\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh, tqdm(total\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mlength, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m chunk \u001b[38;5;241m:=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     31\u001b[0m             fh\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[1;32m     32\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/miniconda3/envs/similarity-exp/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniconda3/envs/similarity-exp/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import STL10\n",
    "from torchvision import models, transforms as T\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# Global paths\n",
    "# -----------------------------\n",
    "ROOT = Path(\".\")                  # project root (adjust if needed)\n",
    "DATA_PATH = ROOT / \"data\"          # where STL-10 will be stored\n",
    "RUNS_DIR = \"runs\"                  # relative to ROOT\n",
    "\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "(ROOT / RUNS_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Device\n",
    "# -----------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# ======================================================\n",
    "# ---------------- CONFIG PARSING ----------------------\n",
    "# ======================================================\n",
    "\n",
    "base_seed = cfg[\"experiment\"][\"base_seed\"]\n",
    "max_runs = cfg[\"experiment\"][\"max_runs\"]\n",
    "\n",
    "subset_size = cfg[\"data\"][\"sample\"][\"subset_size\"]\n",
    "\n",
    "embedding_cfg = cfg[\"embedding\"]\n",
    "batch_size = embedding_cfg[\"batch_size\"]\n",
    "\n",
    "ann_cfg = cfg[\"search\"][\"ann\"][\"parameters\"]\n",
    "M = ann_cfg[\"M\"]\n",
    "ef_construction = ann_cfg[\"efConstruction\"]\n",
    "ef_search = ann_cfg[\"efSearch\"]\n",
    "\n",
    "eval_cfg = cfg[\"evaluation\"]\n",
    "K_VALUES = list(\n",
    "    range(eval_cfg[\"min_k\"], eval_cfg[\"max_k\"] + 1, eval_cfg[\"k_step\"])\n",
    ")\n",
    "\n",
    "EF_SEARCH_VALUES = cfg[\"ef_search_study\"][\"values\"]\n",
    "EF_CONSTRUCTION_VALUES = cfg[\"ef_construction_study\"][\"values\"]\n",
    "ALPHAS = cfg[\"alpha_study\"][\"alpha_values\"]\n",
    "\n",
    "RUN_EF_SEARCH = cfg[\"ef_search_study\"][\"enabled\"]\n",
    "RUN_EF_CONSTRUCTION = cfg[\"ef_construction_study\"][\"enabled\"]\n",
    "RUN_ALPHA = cfg[\"alpha_study\"][\"enabled\"]\n",
    "\n",
    "EF_MIN = 32\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# ---------------- ONE-TIME SETUP ----------------------\n",
    "# ======================================================\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(96),\n",
    "    T.CenterCrop(96),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "dataset = STL10(\n",
    "    root=DATA_PATH,\n",
    "    split=\"unlabeled\",\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "FULL_DATASET_LEN = len(dataset)\n",
    "print(\"Full STL-10 size:\", FULL_DATASET_LEN)\n",
    "\n",
    "resnet = models.resnet50(\n",
    "    weights=models.ResNet50_Weights.IMAGENET1K_V2\n",
    ")\n",
    "embedding_model = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "embedding_model.to(DEVICE)\n",
    "embedding_model.eval()\n",
    "\n",
    "def extract_embeddings(batch):\n",
    "    with torch.no_grad():\n",
    "        feats = embedding_model(batch.to(DEVICE))\n",
    "        feats = feats.squeeze(-1).squeeze(-1)\n",
    "    return feats.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "def to_json_safe(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: to_json_safe(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [to_json_safe(v) for v in obj]\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.floating, np.integer)):\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def run_ann_condition(embeddings, index_ann, D_exact_full, I_exact_full):\n",
    "    results = {\n",
    "        \"k\": [],\n",
    "        \"mean_overlap\": [],\n",
    "        \"mean_exact_dist\": [],\n",
    "        \"mean_ann_dist\": [],\n",
    "        \"mean_barycenter_shift\": [],\n",
    "        \"mean_lid_diff\": [],\n",
    "    }\n",
    "\n",
    "    for k in K_VALUES:\n",
    "        D_exact = D_exact_full[:, 1:k+1]\n",
    "        I_exact = I_exact_full[:, 1:k+1]\n",
    "\n",
    "        D_ann, I_ann = index_ann.search(embeddings, k + 1)\n",
    "        D_ann = D_ann[:, 1:]\n",
    "        I_ann = I_ann[:, 1:]\n",
    "\n",
    "        mean_ov, _ = compute_overlap_stats(I_exact, I_ann, k)\n",
    "        dist_stats = compute_distance_stats(D_exact, D_ann)\n",
    "        bary_shift, _ = compute_barycenter_stats(\n",
    "            embeddings, I_exact, I_ann, D_exact\n",
    "        )\n",
    "        lid_stats = compute_lid_stats(D_exact, D_ann)\n",
    "\n",
    "        results[\"k\"].append(k)\n",
    "        results[\"mean_overlap\"].append(mean_ov)\n",
    "        results[\"mean_exact_dist\"].append(dist_stats[\"mean_exact_dist\"])\n",
    "        results[\"mean_ann_dist\"].append(dist_stats[\"mean_ann_dist\"])\n",
    "        results[\"mean_barycenter_shift\"].append(bary_shift)\n",
    "        results[\"mean_lid_diff\"].append(lid_stats[\"mean_lid_diff\"])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# ---------------- PER-RUN LOOP ------------------------\n",
    "# ======================================================\n",
    "\n",
    "for run_id in range(max_runs):\n",
    "\n",
    "    print(f\"\\n================ RUN {run_id} ================\")\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_dir = ROOT / RUNS_DIR / f\"{ts}_run{run_id:02d}\"\n",
    "    run_dir.mkdir(exist_ok=False)\n",
    "\n",
    "    # ---- Sampling ----\n",
    "    rng = np.random.default_rng(base_seed + run_id)\n",
    "    indices = rng.choice(FULL_DATASET_LEN, subset_size, replace=False)\n",
    "    subset = Subset(dataset, indices)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # ---- Embeddings ----\n",
    "    emb_path = run_dir / \"embeddings.npy\"\n",
    "    if emb_path.exists():\n",
    "        embeddings = np.load(emb_path)\n",
    "    else:\n",
    "        chunks = []\n",
    "        for batch, _ in tqdm(loader):\n",
    "            chunks.append(extract_embeddings(batch))\n",
    "        embeddings = np.vstack(chunks)\n",
    "        np.save(emb_path, embeddings)\n",
    "\n",
    "    d = embeddings.shape[1]\n",
    "\n",
    "    # ---- Exact index ----\n",
    "    index_exact = faiss.IndexFlatL2(d)\n",
    "    index_exact.add(embeddings)\n",
    "\n",
    "    exact_path = run_dir / \"exact_neighbors.npz\"\n",
    "    if exact_path.exists():\n",
    "        data = np.load(exact_path)\n",
    "        D_exact_full = data[\"D\"]\n",
    "        I_exact_full = data[\"I\"]\n",
    "    else:\n",
    "        D_exact_full, I_exact_full = index_exact.search(\n",
    "            embeddings, len(embeddings)\n",
    "        )\n",
    "        np.savez(exact_path, D=D_exact_full, I=I_exact_full)\n",
    "\n",
    "    # ---- Baseline ANN index ----\n",
    "    index_ann = faiss.IndexHNSWFlat(d, M, faiss.METRIC_L2)\n",
    "    index_ann.hnsw.efConstruction = ef_construction\n",
    "    index_ann.hnsw.efSearch = ef_search\n",
    "    index_ann.add(embeddings)\n",
    "\n",
    "    # ==================================================\n",
    "    # Experiment 1: fixed efSearch\n",
    "    # ==================================================\n",
    "    if RUN_EF_SEARCH:\n",
    "        base_dir = run_dir / \"efSearch_study\"\n",
    "        base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        for ef in EF_SEARCH_VALUES:\n",
    "            index_ann.hnsw.efSearch = ef\n",
    "            results = run_ann_condition(\n",
    "                embeddings, index_ann,\n",
    "                D_exact_full, I_exact_full\n",
    "            )\n",
    "            out = base_dir / f\"ef_{ef}\"\n",
    "            out.mkdir(exist_ok=True)\n",
    "            json.dump(\n",
    "                to_json_safe(results),\n",
    "                open(out / \"metrics.json\", \"w\"),\n",
    "                indent=2\n",
    "            )\n",
    "\n",
    "    # ==================================================\n",
    "    # Experiment 2: efConstruction sensitivity\n",
    "    # ==================================================\n",
    "    if RUN_EF_CONSTRUCTION:\n",
    "        base_dir = run_dir / \"efConstruction_study\"\n",
    "        base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        for efc in EF_CONSTRUCTION_VALUES:\n",
    "            index_ann = faiss.IndexHNSWFlat(d, M, faiss.METRIC_L2)\n",
    "            index_ann.hnsw.efConstruction = efc\n",
    "            index_ann.hnsw.efSearch = ef_search\n",
    "            index_ann.add(embeddings)\n",
    "\n",
    "            results = run_ann_condition(\n",
    "                embeddings, index_ann,\n",
    "                D_exact_full, I_exact_full\n",
    "            )\n",
    "            out = base_dir / f\"efc_{efc}\"\n",
    "            out.mkdir(exist_ok=True)\n",
    "            json.dump(\n",
    "                to_json_safe(results),\n",
    "                open(out / \"metrics.json\", \"w\"),\n",
    "                indent=2\n",
    "            )\n",
    "\n",
    "    # ==================================================\n",
    "    # Experiment 3: alpha study\n",
    "    # ==================================================\n",
    "    if RUN_ALPHA:\n",
    "        base_dir = run_dir / \"alpha_study\"\n",
    "        base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        for alpha in ALPHAS:\n",
    "            results = {\n",
    "                \"alpha\": alpha,\n",
    "                \"k\": [],\n",
    "                \"mean_overlap\": [],\n",
    "                \"mean_exact_dist\": [],\n",
    "                \"mean_ann_dist\": [],\n",
    "                \"mean_barycenter_shift\": [],\n",
    "                \"mean_lid_diff\": [],\n",
    "            }\n",
    "\n",
    "            for k in K_VALUES:\n",
    "                ef = max(EF_MIN, int(alpha * k))\n",
    "                index_ann.hnsw.efSearch = ef\n",
    "\n",
    "                D_exact = D_exact_full[:, 1:k+1]\n",
    "                I_exact = I_exact_full[:, 1:k+1]\n",
    "\n",
    "                D_ann, I_ann = index_ann.search(embeddings, k + 1)\n",
    "                D_ann = D_ann[:, 1:]\n",
    "                I_ann = I_ann[:, 1:]\n",
    "\n",
    "                mean_ov, _ = compute_overlap_stats(I_exact, I_ann, k)\n",
    "                dist_stats = compute_distance_stats(D_exact, D_ann)\n",
    "                bary_shift, _ = compute_barycenter_stats(\n",
    "                    embeddings, I_exact, I_ann, D_exact\n",
    "                )\n",
    "                lid_stats = compute_lid_stats(D_exact, D_ann)\n",
    "\n",
    "                results[\"k\"].append(k)\n",
    "                results[\"mean_overlap\"].append(mean_ov)\n",
    "                results[\"mean_exact_dist\"].append(dist_stats[\"mean_exact_dist\"])\n",
    "                results[\"mean_ann_dist\"].append(dist_stats[\"mean_ann_dist\"])\n",
    "                results[\"mean_barycenter_shift\"].append(bary_shift)\n",
    "                results[\"mean_lid_diff\"].append(lid_stats[\"mean_lid_diff\"])\n",
    "\n",
    "            out = base_dir / f\"alpha_{alpha}\"\n",
    "            out.mkdir(exist_ok=True)\n",
    "            json.dump(\n",
    "                to_json_safe(results),\n",
    "                open(out / \"metrics.json\", \"w\"),\n",
    "                indent=2\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3833e151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d385358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "similarity-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
