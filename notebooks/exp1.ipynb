{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a62407a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=RuntimeWarning,\n",
    "    message=\".*overflow.*|.*invalid value.*\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff816d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import faiss\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import STL10\n",
    "from torchvision import models\n",
    "import os \n",
    "import json\n",
    "\n",
    "from src.config import load_config\n",
    "from src.metrics import (\n",
    "    compute_overlap_stats,\n",
    "    compute_distance_stats, \n",
    "    compute_lid_stats,\n",
    "    compute_barycenter_stats,\n",
    ")\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7581792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment': {'name': 'neighborhood_analysis',\n",
       "  'base_seed': 42,\n",
       "  'max_runs': 25,\n",
       "  'min_runs': 10,\n",
       "  'convergence': {'enabled': True,\n",
       "   'window': 5,\n",
       "   'tolerance_abs': '1e-3',\n",
       "   'tolerance_std': '1e-4'}},\n",
       " 'data': {'dataset': 'STL-10',\n",
       "  'total_size': 100000,\n",
       "  'sample': {'enabled': True,\n",
       "   'subset_size': 10000,\n",
       "   'sampling': 'random',\n",
       "   'replacement': False}},\n",
       " 'embedding': {'model': 'resnet50',\n",
       "  'pretrained': 'imagenet1k',\n",
       "  'output_dim': 2048,\n",
       "  'batch_size': 32,\n",
       "  'device': 'cuda',\n",
       "  'normalize': False},\n",
       " 'search': {'distance_metric': 'l2',\n",
       "  'exact': {'enabled': True},\n",
       "  'ann': {'enabled': True,\n",
       "   'algorithm': 'hnsw',\n",
       "   'parameters': {'M': 32, 'efConstruction': 200, 'efSearch': 50}}},\n",
       " 'evaluation': {'min_k': 10,\n",
       "  'max_k': 500,\n",
       "  'k_step': 10,\n",
       "  'metrics': {'neighborhood_overlap': True,\n",
       "   'average_neighbor_distance': True,\n",
       "   'barycenter_shift': True,\n",
       "   'local_intrinsic_dimensionality': True}},\n",
       " 'ef_search_study': {'enabled': True, 'values': [20, 50, 100]},\n",
       " 'ef_construction_study': {'enabled': True,\n",
       "  'values': [50, 100, 200, 300, 400]},\n",
       " 'alpha_study': {'enabled': True,\n",
       "  'single_run': True,\n",
       "  'alpha_values': [0.25, 0.5, 1.0, 2.0, 4.0]},\n",
       " 'output': {'save_embeddings': True,\n",
       "  'save_neighbors': False,\n",
       "  'save_metrics': True,\n",
       "  'save_plots': True,\n",
       "  'format': 'json'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to config\n",
    "CONFIG_PATH = Path(\"../config/config.yaml\")\n",
    "\n",
    "# Load YAML config\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d96539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Full STL-10 size: 100000\n",
      "\n",
      "================ RUN 0 ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:12<00:00, 25.07it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 295\u001b[0m\n\u001b[1;32m    293\u001b[0m mean_ov, _ \u001b[38;5;241m=\u001b[39m compute_overlap_stats(I_exact, I_ann, k)\n\u001b[1;32m    294\u001b[0m dist_stats \u001b[38;5;241m=\u001b[39m compute_distance_stats(D_exact, D_ann)\n\u001b[0;32m--> 295\u001b[0m bary_shift, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_barycenter_stats\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mI_exact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mI_ann\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD_exact\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m lid_stats \u001b[38;5;241m=\u001b[39m compute_lid_stats(D_exact, D_ann)\n\u001b[1;32m    300\u001b[0m lid_exact \u001b[38;5;241m=\u001b[39m lid_stats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_lid_exact\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/MSDS Research/NeighbourhoodDriftInImageEmbeddings/src/metrics.py:126\u001b[0m, in \u001b[0;36mcompute_barycenter_stats\u001b[0;34m(embeddings, I_exact, I_ann, D_exact)\u001b[0m\n\u001b[1;32m    123\u001b[0m ann_neighbors \u001b[38;5;241m=\u001b[39m embeddings[I_ann[i]]\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Barycenters\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m c_exact \u001b[38;5;241m=\u001b[39m \u001b[43mexact_neighbors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m c_ann \u001b[38;5;241m=\u001b[39m ann_neighbors\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Absolute shift\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/similarity-exp/lib/python3.10/site-packages/numpy/_core/_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m um\u001b[38;5;241m.\u001b[39mclip(a, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_mean\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    119\u001b[0m     arr \u001b[38;5;241m=\u001b[39m asanyarray(a)\n\u001b[1;32m    121\u001b[0m     is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import STL10\n",
    "from torchvision import models, transforms as T\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# Global paths\n",
    "# -----------------------------\n",
    "ROOT = Path(\".\")                  # project root (adjust if needed)\n",
    "DATA_PATH = ROOT / \"../data\"          # where STL-10 will be stored\n",
    "RUNS_DIR = \"../runs\"                  # relative to ROOT\n",
    "\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "(ROOT / RUNS_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Device\n",
    "# -----------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# ======================================================\n",
    "# ---------------- CONFIG PARSING ----------------------\n",
    "# ======================================================\n",
    "\n",
    "base_seed = cfg[\"experiment\"][\"base_seed\"]\n",
    "max_runs = cfg[\"experiment\"][\"max_runs\"]\n",
    "\n",
    "subset_size = cfg[\"data\"][\"sample\"][\"subset_size\"]\n",
    "\n",
    "embedding_cfg = cfg[\"embedding\"]\n",
    "batch_size = embedding_cfg[\"batch_size\"]\n",
    "\n",
    "ann_cfg = cfg[\"search\"][\"ann\"][\"parameters\"]\n",
    "M = ann_cfg[\"M\"]\n",
    "ef_construction = ann_cfg[\"efConstruction\"]\n",
    "ef_search = ann_cfg[\"efSearch\"]\n",
    "\n",
    "eval_cfg = cfg[\"evaluation\"]\n",
    "K_VALUES = list(\n",
    "    range(eval_cfg[\"min_k\"], eval_cfg[\"max_k\"] + 1, eval_cfg[\"k_step\"])\n",
    ")\n",
    "\n",
    "EF_SEARCH_VALUES = cfg[\"ef_search_study\"][\"values\"]\n",
    "EF_CONSTRUCTION_VALUES = cfg[\"ef_construction_study\"][\"values\"]\n",
    "ALPHAS = cfg[\"alpha_study\"][\"alpha_values\"]\n",
    "\n",
    "RUN_EF_SEARCH = cfg[\"ef_search_study\"][\"enabled\"]\n",
    "RUN_EF_CONSTRUCTION = cfg[\"ef_construction_study\"][\"enabled\"]\n",
    "RUN_ALPHA = cfg[\"alpha_study\"][\"enabled\"]\n",
    "\n",
    "EF_MIN = 32\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# ---------------- ONE-TIME SETUP ----------------------\n",
    "# ======================================================\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "            std=(0.229, 0.224, 0.225))\n",
    "\n",
    "])\n",
    "\n",
    "dataset = STL10(\n",
    "    root=DATA_PATH,\n",
    "    split=\"unlabeled\",\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "FULL_DATASET_LEN = len(dataset)\n",
    "print(\"Full STL-10 size:\", FULL_DATASET_LEN)\n",
    "\n",
    "resnet = models.resnet50(\n",
    "    weights=models.ResNet50_Weights.IMAGENET1K_V2\n",
    ")\n",
    "embedding_model = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "embedding_model.to(DEVICE)\n",
    "embedding_model.eval()\n",
    "\n",
    "def extract_embeddings(batch):\n",
    "    with torch.no_grad():\n",
    "        feats = embedding_model(batch.to(DEVICE))\n",
    "        feats = feats.squeeze(-1).squeeze(-1)\n",
    "    return feats.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "def to_json_safe(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: to_json_safe(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [to_json_safe(v) for v in obj]\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.floating, np.integer)):\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def run_ann_condition(embeddings, index_ann, D_exact_full, I_exact_full):\n",
    "    results = {\n",
    "        \"k\": [],\n",
    "        \"mean_overlap\": [],\n",
    "        \"mean_exact_dist\": [],\n",
    "        \"mean_ann_dist\": [],\n",
    "        \"mean_barycenter_shift\": [],\n",
    "        \"mean_lid_diff\": [],\n",
    "        \"mean_lid_exact\": [],\n",
    "        \"mean_lid_ann\": [],\n",
    "    }\n",
    "\n",
    "    for k in K_VALUES:\n",
    "        D_exact = D_exact_full[:, 1:k+1]\n",
    "        I_exact = I_exact_full[:, 1:k+1]\n",
    "\n",
    "        D_ann, I_ann = index_ann.search(embeddings, k + 1)\n",
    "        D_ann = D_ann[:, 1:]\n",
    "        I_ann = I_ann[:, 1:]\n",
    "\n",
    "        mean_ov, _ = compute_overlap_stats(I_exact, I_ann, k)\n",
    "        dist_stats = compute_distance_stats(D_exact, D_ann)\n",
    "        bary_shift, _ = compute_barycenter_stats(\n",
    "            embeddings, I_exact, I_ann, D_exact\n",
    "        )\n",
    "        lid_stats = compute_lid_stats(D_exact, D_ann)\n",
    "\n",
    "        lid_exact = lid_stats[\"mean_lid_exact\"]\n",
    "        lid_ann = lid_stats[\"mean_lid_ann\"]\n",
    "        lid_diff = lid_ann - lid_exact\n",
    "\n",
    "        results[\"k\"].append(k)\n",
    "        results[\"mean_overlap\"].append(mean_ov)\n",
    "        results[\"mean_exact_dist\"].append(dist_stats[\"mean_exact_dist\"])\n",
    "        results[\"mean_ann_dist\"].append(dist_stats[\"mean_ann_dist\"])\n",
    "        results[\"mean_barycenter_shift\"].append(bary_shift)\n",
    "        results[\"mean_lid_exact\"].append(lid_exact)\n",
    "        results[\"mean_lid_ann\"].append(lid_ann)\n",
    "        results[\"mean_lid_diff\"].append(lid_diff)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# ---------------- PER-RUN LOOP ------------------------\n",
    "# ======================================================\n",
    "\n",
    "for run_id in range(max_runs):\n",
    "\n",
    "    print(f\"\\n================ RUN {run_id} ================\")\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_dir = ROOT / RUNS_DIR / f\"{ts}_run{run_id:02d}\"\n",
    "    run_dir.mkdir(exist_ok=False)\n",
    "\n",
    "    # ---- Sampling ----\n",
    "    rng = np.random.default_rng(base_seed + run_id)\n",
    "    indices = rng.choice(FULL_DATASET_LEN, subset_size, replace=False)\n",
    "    subset = Subset(dataset, indices)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # ---- Embeddings ----\n",
    "    emb_path = run_dir / \"embeddings.npy\"\n",
    "    if emb_path.exists():\n",
    "        embeddings = np.load(emb_path)\n",
    "    else:\n",
    "        chunks = []\n",
    "        for batch, _ in tqdm(loader):\n",
    "            chunks.append(extract_embeddings(batch))\n",
    "        embeddings = np.vstack(chunks)\n",
    "        np.save(emb_path, embeddings)\n",
    "\n",
    "    d = embeddings.shape[1]\n",
    "\n",
    "    # ---- Exact index ----\n",
    "    index_exact = faiss.IndexFlatL2(d)\n",
    "    index_exact.add(embeddings)\n",
    "\n",
    "    exact_path = run_dir / \"exact_neighbors.npz\"\n",
    "    if exact_path.exists():\n",
    "        data = np.load(exact_path)\n",
    "        D_exact_full = data[\"D\"]\n",
    "        I_exact_full = data[\"I\"]\n",
    "    else:\n",
    "        D_exact_full, I_exact_full = index_exact.search(\n",
    "            embeddings, len(embeddings)\n",
    "        )\n",
    "        np.savez(exact_path, D=D_exact_full, I=I_exact_full)\n",
    "\n",
    "    # ---- Baseline ANN index ----\n",
    "    index_ann = faiss.IndexHNSWFlat(d, M, faiss.METRIC_L2)\n",
    "    index_ann.hnsw.efConstruction = ef_construction\n",
    "    index_ann.hnsw.efSearch = ef_search\n",
    "    index_ann.add(embeddings)\n",
    "\n",
    "    # ==================================================\n",
    "    # Experiment 1: fixed efSearch\n",
    "    # ==================================================\n",
    "    if RUN_EF_SEARCH:\n",
    "        base_dir = run_dir / \"efSearch_study\"\n",
    "        base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        for ef in EF_SEARCH_VALUES:\n",
    "            index_ann.hnsw.efSearch = ef\n",
    "            results = run_ann_condition(\n",
    "                embeddings, index_ann,\n",
    "                D_exact_full, I_exact_full\n",
    "            )\n",
    "            out = base_dir / f\"ef_{ef}\"\n",
    "            out.mkdir(exist_ok=True)\n",
    "            json.dump(\n",
    "                to_json_safe(results),\n",
    "                open(out / \"metrics.json\", \"w\"),\n",
    "                indent=2\n",
    "            )\n",
    "\n",
    "    # ==================================================\n",
    "    # Experiment 2: efConstruction sensitivity\n",
    "    # ==================================================\n",
    "    if RUN_EF_CONSTRUCTION:\n",
    "        base_dir = run_dir / \"efConstruction_study\"\n",
    "        base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        for efc in EF_CONSTRUCTION_VALUES:\n",
    "            index_ann = faiss.IndexHNSWFlat(d, M, faiss.METRIC_L2)\n",
    "            index_ann.hnsw.efConstruction = efc\n",
    "            index_ann.hnsw.efSearch = ef_search\n",
    "            index_ann.add(embeddings)\n",
    "\n",
    "            results = run_ann_condition(\n",
    "                embeddings, index_ann,\n",
    "                D_exact_full, I_exact_full\n",
    "            )\n",
    "            out = base_dir / f\"efc_{efc}\"\n",
    "            out.mkdir(exist_ok=True)\n",
    "            json.dump(\n",
    "                to_json_safe(results),\n",
    "                open(out / \"metrics.json\", \"w\"),\n",
    "                indent=2\n",
    "            )\n",
    "\n",
    "    # ==================================================\n",
    "    # Experiment 3: alpha study\n",
    "    # ==================================================\n",
    "    if RUN_ALPHA:\n",
    "        base_dir = run_dir / \"alpha_study\"\n",
    "        base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        for alpha in ALPHAS:\n",
    "            results = {\n",
    "                \"alpha\": alpha,\n",
    "                \"k\": [],\n",
    "                \"mean_overlap\": [],\n",
    "                \"mean_exact_dist\": [],\n",
    "                \"mean_ann_dist\": [],\n",
    "                \"mean_barycenter_shift\": [],\n",
    "                \"mean_lid_diff\": [],\n",
    "                \"mean_lid_exact\": [],\n",
    "                \"mean_lid_ann\": [],\n",
    "            }\n",
    "\n",
    "            for k in K_VALUES:\n",
    "                ef = max(EF_MIN, int(alpha * k))\n",
    "                index_ann.hnsw.efSearch = ef\n",
    "\n",
    "                D_exact = D_exact_full[:, 1:k+1]\n",
    "                I_exact = I_exact_full[:, 1:k+1]\n",
    "\n",
    "                D_ann, I_ann = index_ann.search(embeddings, k + 1)\n",
    "                D_ann = D_ann[:, 1:]\n",
    "                I_ann = I_ann[:, 1:]\n",
    "\n",
    "                mean_ov, _ = compute_overlap_stats(I_exact, I_ann, k)\n",
    "                dist_stats = compute_distance_stats(D_exact, D_ann)\n",
    "                bary_shift, _ = compute_barycenter_stats(\n",
    "                    embeddings, I_exact, I_ann, D_exact\n",
    "                )\n",
    "                lid_stats = compute_lid_stats(D_exact, D_ann)\n",
    "\n",
    "                lid_exact = lid_stats[\"mean_lid_exact\"]\n",
    "                lid_ann = lid_stats[\"mean_lid_ann\"]\n",
    "                lid_diff = lid_ann - lid_exact\n",
    "\n",
    "                results[\"k\"].append(k)\n",
    "                results[\"mean_overlap\"].append(mean_ov)\n",
    "                results[\"mean_exact_dist\"].append(dist_stats[\"mean_exact_dist\"])\n",
    "                results[\"mean_ann_dist\"].append(dist_stats[\"mean_ann_dist\"])\n",
    "                results[\"mean_barycenter_shift\"].append(bary_shift)\n",
    "                results[\"mean_lid_diff\"].append(lid_diff)\n",
    "                results[\"mean_lid_ann\"].append(lid_ann)\n",
    "                results[\"mean_lid_exact\"].append(lid_diff)\n",
    "\n",
    "\n",
    "            out = base_dir / f\"alpha_{alpha}\"\n",
    "            out.mkdir(exist_ok=True)\n",
    "            json.dump(\n",
    "                to_json_safe(results),\n",
    "                open(out / \"metrics.json\", \"w\"),\n",
    "                indent=2\n",
    "            )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "similarity-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
